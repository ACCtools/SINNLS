{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Julia version code\n",
    "\"\"\"\n",
    "nonnegative linear regression \n",
    "\"\"\"\n",
    "# Discussion from Dec 22\n",
    "# 1. Currently, our runtime is O(n/sqrt(eps)*(m+n)). The per iteration O(m)\n",
    "# is unavoidable; however, IF we are NOT required to output the optimizer xtilde_ktotal, \n",
    "# then by maintaining 1^T x in each iteration (at a cost of O(1)), we can completely\n",
    "# avoid O(n) per iteration. \n",
    "\n",
    "using LinearAlgebra, BenchmarkTools, Plots, Convex, SCS, NonNegLeastSquares, MLDatasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "alg_our_core (generic function with 3 methods)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function alg_ours_with_restart(C::Matrix{Float64}, b::Matrix{Float64}, ϵ::Float64 )\n",
    "    extra_term_nnls = 0.5*norm(b)^2\n",
    "    m, n = size(C)\n",
    "    #number of times we restart\n",
    "    K = 10 #ceil(log2(1/ϵ)); We use numbers to test out restart based on iteration count rather than a metric. \n",
    "    col_norm = norm.(eachcol(C))\n",
    "    inv_col_norm_square = 1.0 ./(col_norm.^2)\n",
    "    idx_seq = 1:n\n",
    "    \n",
    "    x0 = zeros(n)\n",
    "    y0 = zeros(m)\n",
    "    z0 = zeros(m)\n",
    "    \n",
    "    \n",
    "    gamma_scaling = 1\n",
    "    \n",
    "    for i=1:K\n",
    "        gamma_scaling+=1 \n",
    "        gamma = 50/gamma_scaling \n",
    "        xktilde, yktilde, zktilde = alg_our_core(C, x0, y0, z0, m, n, inv_col_norm_square, idx_seq, ϵ, gamma)\n",
    "        x0[:] = xktilde[:]\n",
    "        y0[:] = yktilde[:]\n",
    "        z0[:] = zktilde[:]\n",
    "        \n",
    "    end\n",
    "    return -sum(x0) + 0.5* norm(z0)^2 +extra_term_nnls\n",
    "end\n",
    "\n",
    "function alg_our_core(C, x0, y0, z0, m, n, inv_col_norm_square, idx_seq, ϵ, gamma)\n",
    "        # reset all the scaling factors \n",
    "        previous_A = 1.0/n\n",
    "        previous_a = previous_A #a_1, A_1\n",
    "        a = 1.0/(n*n) # a_2\n",
    "        A = (n+1.0)/(n * n) # A_2\n",
    "        \n",
    "        # compute x1 using the input x0 \n",
    "        # we redefined phio(x) = 1/2 * ||x-x0||_A^2, hence updating x requires x0 \n",
    "        # the step p(j)+=1/||A:j||^2 implicitly assumes ybar_0 = 0 \n",
    "        # To allow for ybar_0 \\neq 0, we change p(j) a bit \n",
    "       \n",
    "        ybar = copy(y0)\n",
    "        j = rand(idx_seq)\n",
    "        Aty0m = 1 - dot(ybar, C[:, j]) #                                     dot(̄ȳ, C[:, j])\n",
    "    \n",
    "        p = copy(x0) \n",
    "        x = copy(x0)\n",
    "        p[j] += inv_col_norm_square[j]*Aty0m \n",
    "        x[j] = min(inv_col_norm_square[j], max(0, p[j])) #x and x0 differ only at j \n",
    "        \n",
    "        # compute y1\n",
    "        # note that y0^(R) and y1^(R) are independent of each othre\n",
    "        # y1^(R) = Ax1^(R) = Ax0^(R) + A*(x1^R - x0^R) = z0^R + A*(x1^R - x0^R)\n",
    "        # y0^R may be chosen to be either ytildeK or 0 (our analysis uses 0)\n",
    "        # Further note that if y0^R = ytildeK, then we must ALSO choose ybar_0 = ytildeK, and \n",
    "        # this changes how x is init. \n",
    "        previous_y = copy(y0) \n",
    "        z = copy(z0) \n",
    "        z += C[:, j] * (x[j] - x0[j]) # z_1 = A x_1 = A (x_0 + (x_1 - x_0))\n",
    "        y = copy(z) # y_1 = A xtilde1 = A x_1 = z_1 \n",
    "    \n",
    "        # compute ȳ, ỹ (because we need to return it), and some auxiliary variables \n",
    "        ybar[:] = y[:] + previous_a/a * (y[:] - previous_y[:]) #ybar_1 \n",
    "        s = zeros(n) # need this so that xtildek = xk + sk/Ak; s_1 = 0 (see Chaobing's lemma for why this is needed)\n",
    "        ỹ = copy(y) # ytildek = convex comb of yi's, so ytilde1 = y1\n",
    "\n",
    "        # restart value init; -1^{\\top}x+0.5\\|Ax\\|^{2}+.5*\\|y\\|^{2}+\\frac{1}{2\\epsilon}\\|(-A^{\\top}y+1)^{+}\\|^{2}\n",
    "        restart_coeff = 5000\n",
    "        Atym = -C'*y0 .+ 1 \n",
    "        truncated_Atym = ((Atym) .> 0).*Atym\n",
    "        restart_val_prev = -sum(x0)+ 0.5* norm(z0)^2 +0.5*norm(y0)^2 + restart_coeff*norm(truncated_Atym)^2\n",
    "        restart_val_curr = restart_val_prev\n",
    "            \n",
    "        # inits for restart\n",
    "        iter_count = 0 \n",
    "        Flag = true\n",
    "        Ax0 = zeros(m)\n",
    "\n",
    "        while (Flag)\n",
    "\n",
    "            # updates related to x\n",
    "            j = rand(idx_seq)\n",
    "            p[j] += - n * inv_col_norm_square[j] * a * (sum(C[:,j] .* ybar) - 1)\n",
    "            prev_xj = x[j]\n",
    "            x[j] = min(inv_col_norm_square[j], max(0, p[j]))\n",
    "            # update s so that we may return xtildek at only O(1) cost\n",
    "            s[j] += ((n-1) * a -  previous_A) * (x[j] - prev_xj)\n",
    "        \n",
    "            # updates related to y \n",
    "            previous_y[:] = y[:]\n",
    "            z[:] += C[:, j] * (x[j] - prev_xj)\n",
    "            y[:] = previous_A/A * y[:] + a/A * z[:] + (n-1) * a/A * (x[j] - prev_xj) * C[:,j]\n",
    "            # need to update ytilde each time because that's what we want to return, \n",
    "            # and we aren't saving all the yi's. \n",
    "            ỹ[:] = previous_A/A * ỹ[:] + a/A * y[:]\n",
    "        \n",
    "            # update scaling factors \n",
    "            previous_a, previous_A = a, A\n",
    "            a = min(n * a/(n-1), sqrt(A)/(2*n))\n",
    "            A += a\n",
    "        \n",
    "            # update ȳ (note that ȳ_k depends on a_k and a_{k+1})\n",
    "            ybar[:] = y[:] + previous_a/a * (y[:] - previous_y[:])\n",
    "        \n",
    "            # restart stuff \n",
    "            iter_count+=1\n",
    "            # Since we are computing the restart condition without any optimizations, \n",
    "            # and the restart condition likely involves (expensive) matrix-vector products, \n",
    "            # we check it only after a certain number of iters have passed. \n",
    "\n",
    "            if ((restart_val_curr < 0)|| (iter_count% ceil(n*gamma) ==0))\n",
    "                # compute the restart condition \n",
    "                Atym = -C'*ỹ .+ 1 \n",
    "                truncated_Atym = ((Atym) .> 0).*Atym  \n",
    "                sumx0 = sum(x + (1.0/previous_A) * s)\n",
    "                Ax0 = C*(x + (1.0/previous_A) * s)\n",
    "                restart_val_curr = -sumx0+ 0.5* norm(Ax0)^2 +0.5*norm(ỹ)^2 + restart_coeff*norm(truncated_Atym)^2\n",
    "                if (restart_val_curr < 0)\n",
    "                    Flag = false\n",
    "                end\n",
    "                # For now, we want to see if restarting works or not without checking the metric \n",
    "                # and therefore we force restart by artificially fixing the Flag to false. \n",
    "                # Once we can get restart to work, we'll put the Flag = false inside the \n",
    "                # if condition of restart_metric. \n",
    "                if (restart_val_curr <= 0.5*restart_val_prev)\n",
    "                   # print(\"curr = \", restart_val_curr, \", prev = \", restart_val_prev, \"obj = \", -sumx0+ 0.5* norm(Ax0)^2,\"\\n\")\n",
    "                    Flag = false\n",
    "                end\n",
    "            end\n",
    "            \n",
    "        end\n",
    "        return x + (1.0/previous_A) * s, ỹ, Ax0\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "alg_ours_without_restart (generic function with 1 method)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function alg_ours_without_restart(C::Matrix{Float64}, b::Matrix{Float64}, ϵ::Float64)\n",
    "    \n",
    "    extra_term_nnls = 0.5*norm(b)^2\n",
    "    m, n = size(C)\n",
    "    K = ceil(n / √ϵ)\n",
    "    previous_A = 1.0/n\n",
    "    previous_a = previous_A\n",
    "    a = 1.0/(n*n)\n",
    "    A = (n+1.0) /(n * n)\n",
    "    col_norm = norm.(eachcol(C))\n",
    "    inv_col_norm_square = 1.0 ./(col_norm.^2)\n",
    "    idx_seq = 1:n\n",
    "    x = zeros(n)\n",
    "    p = zeros(n)\n",
    "    j = rand(idx_seq)\n",
    "    p[j] += inv_col_norm_square[j]\n",
    "    x[j] = p[j]\n",
    "    # x̃ = deepcopy(x)\n",
    "    previous_y = zeros(m)\n",
    "    y = x[j] * C[:, j]\n",
    "    # record Ax\n",
    "    z = x[j] * C[:, j]\n",
    "    ȳ = (n+1) * y\n",
    "    s = zeros(n)\n",
    "    #func_value = 0\n",
    "    func_value=zeros(Int(ceil(K/n)))\n",
    "    \n",
    "    for k = 2:K\n",
    "        j = rand(idx_seq)\n",
    "        p[j] += - n * inv_col_norm_square[j] * a * (sum(C[:,j] .* ȳ) - 1)\n",
    "        prev_xj = x[j]\n",
    "        x[j] = min(inv_col_norm_square[j], max(0, p[j]))\n",
    "        # record Ax\n",
    "        z[:] += C[:, j] * (x[j] - prev_xj)\n",
    "        previous_y[:] = y[:]\n",
    "        y[:] = previous_A/A * y[:] + a/A * z[:] + (n-1) * a/A * (x[j] - prev_xj) * C[:,j]\n",
    "        s[j] += ((n-1) * a -  previous_A) * (x[j] - prev_xj)\n",
    "        previous_a, previous_A = a, A\n",
    "        a = min(n * a/(n-1), sqrt(A)/(2*n))\n",
    "        ȳ[:] = y[:] + previous_a/a * (y[:] - previous_y[:])\n",
    "        A += a\n",
    "\n",
    "        if k % n == 0\n",
    "            x̃ = x + 1.0/previous_A * s\n",
    "            C_x̃ = C * x̃\n",
    "            func_value[Int(k/n)] = 0.5 * sum(C_x̃ .* C_x̃) - sum(x̃)+extra_term_nnls\n",
    "            #@info \"pass: $(k/n), func_value: $func_value\"\n",
    "        end\n",
    "    end\n",
    "    return(func_value)\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "alg_lawsonhanson (generic function with 1 method)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/ahwillia/NonNegLeastSquares.jl\n",
    "function alg_lawsonhanson(A, b)\n",
    "    xnnls = nonneg_lsq(A,b;alg=:nnls)  # NNLS\n",
    "\n",
    "    nnls_optval = 0.5*norm(A*xnnls - b)^2\n",
    "    \n",
    "    print(\"\\n nnls package value is \", nnls_optval, \", and time is \")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our result without restart is 0.004256918459444536, and time is  16.048316 seconds (99.30 M allocations: 13.394 GiB, 17.83% gc time)\n",
      "our result is 0.0042568381496070895, and time is,  67.634191 seconds (619.44 M allocations: 83.587 GiB, 15.04% gc time)\n",
      "\n",
      " nnls package value is 0.004256811891241667, and time is   0.423062 seconds (56 allocations: 5.305 MiB)\n"
     ]
    }
   ],
   "source": [
    "# Main code\n",
    "function remove_col1(A,b)#Chenghui has an idea to optimize this for speed (\"filter\")\n",
    "    s=A'*b # n*1 \n",
    "    B=A[:,vec(s.>0)] # m*b matrix where b is smaller than n\n",
    "    s=s[vec(s.>0)] # s is b*1 in dimensions\n",
    "    return B./s'\n",
    "end\n",
    "\n",
    "epsilon = 0.0001 \n",
    "\n",
    "############# \n",
    "n = 50000 # variable dimension \n",
    "m = 10 # Number of data points\n",
    "b=rand(m,1)-repeat([0.3],m,1)\n",
    "A_init =  max.(0, randn(m, n)) #rand(m,n)#\n",
    "############\n",
    "\n",
    "############ Mnist： uncomment \n",
    "#train_x, train_y = MNIST.traindata()\n",
    "#A = Array{Int64}\n",
    "#b = Array{Int64}\n",
    "#A_init = reshape(train_x,60000,28*28)\n",
    "#b = train_y\n",
    "#test_x,  test_y  = MNIST.testdata()\n",
    "############\n",
    "\n",
    "\n",
    " A = remove_col1(A_init,b)\n",
    "############ Mnist: uncomment this\n",
    "# A = Float64.(A)\n",
    "# b = Float64.(b)\n",
    "# b1 = vcat(b')\n",
    "############\n",
    "\n",
    "(m,n) = size(A) # Redefine the size number n and m to prevent triviality.\n",
    "@time begin\n",
    "our_result = alg_ours_without_restart(A, b, epsilon)\n",
    "    print(\"our result without restart is \", our_result[end], \", and time is \")\n",
    "#our_result = alg_ours(A, b1, epsilon)\n",
    "end\n",
    "\n",
    "@time begin\n",
    "our_result = alg_ours_with_restart(A, b, epsilon)\n",
    "print(\"our result is \", our_result, \", and time is, \")\n",
    "#our_result = alg_ours(A, b1, epsilon)\n",
    "end\n",
    "\n",
    "@time begin \n",
    "alg_lawsonhanson(A, b)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 9\n",
    "2^K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.0",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
