{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "remove_col1 (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Julia version code\n",
    "\"\"\"\n",
    "nonnegative linear regression \n",
    "\"\"\"\n",
    "# Discussion from Dec 22\n",
    "# 1. Currently, our runtime is O(n/sqrt(eps)*(m+n)). The per iteration O(m)\n",
    "# is unavoidable; however, IF we are NOT required to output the optimizer xtilde_ktotal, \n",
    "# then by maintaining 1^T x in each iteration (at a cost of O(1)), we can completely\n",
    "# avoid O(n) per iteration. \n",
    "\n",
    "using LinearAlgebra, BenchmarkTools, Distributions, Plots, Convex, SCS\n",
    "const MultivariateDistribution{S<:ValueSupport} = Distribution{Multivariate,S}\n",
    "const DiscreteMultivariateDistribution   = Distribution{Multivariate, Discrete}\n",
    "const ContinuousMultivariateDistribution = Distribution{Multivariate, Continuous}\n",
    "\n",
    "function compute_scaling(A)\n",
    "    scaling_vector = -1 ./sum(abs2.(A),dims=1) # scaling_vector[j] = -1/||A_{:j}||^2\n",
    "    return scaling_vector\n",
    "end\n",
    "\n",
    "function init_all(epsilon, m, n, A)\n",
    "    ktotal = Int(ceil(n/sqrt(epsilon))) #note that this is just an approx ktotal\n",
    "    \n",
    "    akm = 1/n #a_1\n",
    "    Akm = 1/n #A_1\n",
    "    ak = 1/n^2 #a_2\n",
    "    Ak = (1+n)/n^2 #A_2\n",
    "    \n",
    "    scaling_vector = compute_scaling(A) \n",
    "    \n",
    "    randomseed=rand(Multinomial(1, ones(n)/n),1)\n",
    "    jk = findall(vec(randomseed.==1))[1] #find the index at which we have a \"1\"\n",
    "    \n",
    "    v = zeros(n) # vector used to construct x\n",
    "    v[jk] = -1 \n",
    "    xkm = zeros(n) #x_1 \n",
    "    xkm[jk]=-1*scaling_vector[jk] #xkm[jk] = 1/||A_{:jk}||^2\n",
    "    xtildek = deepcopy(xkm) #we need \"copy()\" or \"deepcopy()\" so that modifying xtildek doesn't modify xkm\n",
    "\n",
    "    Axkm = A[:,jk]*xkm[jk]\n",
    "    \n",
    "    ykm = deepcopy(Axkm) # ykm = y_1 \n",
    "    y = deepcopy(ykm) \n",
    "    ybar = (n+1) * ykm # ybar = ybar_1\n",
    "    \n",
    "    return ybar, ykm, y, xtildek, ktotal, ak, akm, Ak, Akm, v, xkm, Axkm, scaling_vector\n",
    "end\n",
    "\n",
    "function update_v(jk, A, ybar, v, ak) \n",
    "    v[jk]+= n*ak*(dot(A[:,jk],ybar) - 1) # ybar = ybar_{k-1}\n",
    "    # no need to return v because the value in memory is changed\n",
    "end\n",
    "\n",
    "function update_x(v, jk, xkm, scaling) \n",
    "    x = deepcopy(xkm)\n",
    "    x[jk] = min(max(scaling*v[jk], 0),-scaling) # n has appeared in v\n",
    "    return x      \n",
    "end\n",
    "\n",
    "function update_y_xtildek_Axkm(jk, n, ak, Ak, Akm, x, xkm, xtildek, Axkm, y, A)\n",
    "    xtildek .= (Akm/Ak)*xtildek+(n*ak/Ak)*x-((n-1)*ak/Ak)*xkm \n",
    "    # we crucially need .= here for xtildek to be updated,\n",
    "    # since we are passing in the memory and changing there. \n",
    "    # Next, we want to compute the following: \n",
    "    # y = A*xtildek. We can optimize this using the following observation. \n",
    "    # A*(x-xkm) = A[:,jk]*(x[jk]-xkm[jk])\n",
    "    temp1 = A[:,jk]*(x[jk]-xkm[jk]) \n",
    "    y .= (Akm/Ak)*y + (n*ak/Ak)*temp1 + (ak/Ak)*Axkm\n",
    "    Axkm .= Axkm+temp1\n",
    "end\n",
    "\n",
    "function update_ak_Ak(Ak, Akm, ak, akm, n) \n",
    "    akm = ak\n",
    "    ak = min(n*ak/(n-1),sqrt(Ak)/(2*n))\n",
    "    Akm = Ak\n",
    "    Ak = Ak+ ak\n",
    "    return Ak, Akm, ak, akm # need to return because they are scalars, not arrays\n",
    "end\n",
    "\n",
    "function update_ybar(y, ykm, ak, akm)\n",
    "    ybar = y + (akm/ak)*(y - ykm)\n",
    "    return ybar \n",
    "end\n",
    "\n",
    "function remove_col1(A,b)#Chenghui has an idea to optimize this for speed (\"filter\")\n",
    "    s=A'*b # n*1 \n",
    "    B=A[:,vec(s.>0)] # m*b matrix where b is smaller than n\n",
    "    s=s[vec(s.>0)] # s is b*1 in dimensions\n",
    "    A .= B./s'\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code is our primal-dual algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "algorithm_ours (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "function algorithm_ours(epsilon, m, n, b, A)\n",
    "    extra_term_nnls = 0.5*norm(b)^2 #since we solve 1/2||Ax||^2 - 1^x, this term must be added to get 1/2 ||Ax - b||^2\n",
    "\n",
    "    (ybar, ykm, y, xtildek, ktotal, ak, akm, Ak, Akm, v, xkm, Axkm, scaling_vector) = init_all(epsilon, m, n, A)\n",
    "\n",
    "    our_result = zeros(ktotal) #we store the objective value in our_result\n",
    "    our_result[1] = norm(ykm)^2/2-sum(xtildek)+extra_term_nnls\n",
    "\n",
    "    for k in 2:ktotal \n",
    "        # sample jk \n",
    "        jk = rand(1:n)\n",
    "\n",
    "        # update v (this is the randomized part in the update of x)\n",
    "        update_v(jk, A, ybar, v, ak) \n",
    "\n",
    "        # update x\n",
    "        x = update_x(v, jk, xkm, scaling_vector[jk])\n",
    "\n",
    "        # Update y and xtildek\n",
    "        update_y_xtildek_Axkm(jk, n, ak, Ak, Akm, x, xkm, xtildek, Axkm, y, A)\n",
    "\n",
    "        #update ak, Ak, Akm, akm \n",
    "        (Ak, Akm, ak, akm) = update_ak_Ak(Ak, Akm, ak, akm, n)\n",
    "\n",
    "        #update ybar \n",
    "        ybar = update_ybar(y, ykm, ak, akm)\n",
    "\n",
    "        # update xkm \n",
    "        xkm = deepcopy(x)\n",
    "        ykm = deepcopy(y)\n",
    "\n",
    "        our_result[k] = norm(y)^2/2-sum(xtildek) + extra_term_nnls #note that we solve min_{x\\geq 0} 0.5 ||Ax||^2 - 1^T x. \n",
    "    end\n",
    "\n",
    "    #our_solution = deepcopy(xtildek)\n",
    "    print(\"\\n Our value is \", our_result[ktotal])\n",
    "    return our_result\n",
    "end \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code is using the Convex.jl package https://jump.dev/Convex.jl/stable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "algorithm_convex_dot_jl (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function algorithm_convex_dot_jl(A, b, n)\n",
    "    x = Variable(n)\n",
    "    problem = minimize(0.5*sumsquares(A * x - b), [x >= 0])\n",
    "    solve!(problem, () -> SCS.Optimizer(verbose=false))\n",
    "    print(\"\\n Convex.jl value is \", problem.optval)\n",
    "end "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code is from the algorithm by Kim-Sra-Dhillon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "algorithm_kim_sra_dhillon (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize \n",
    "function ksd_init(n)\n",
    "    return zeros(n), zeros(n) # we don't know how they init x1 [TBD]\n",
    "end\n",
    "\n",
    "# compute function value f(x) = 1/2||Ax - b||^2\n",
    "function fn_val(A, b, x)\n",
    "    return 0.5*norm(A*x-b)^2\n",
    "end \n",
    "\n",
    "# compute nabla f(x) = A^T (Ax - b)\n",
    "function grad(A, b, x)\n",
    "    u = A*x-b\n",
    "    return A'*u \n",
    "end    \n",
    "# if we precompute AtA = A^T*A, then, we will be doing operations like: AtA*v; this costs O(n^2); \n",
    "# additionally, the one-time cost of computing AtA is O(mn^2). So total cost is O(n^2 (ktotal + m))\n",
    "# OTOH, if we don't precompute, then each time, we will be doing matrix-vector products of the type \n",
    "# A*x or A'*y, both of which cost O(mn). Since we are in the regime m<<n, this is clearly better. \n",
    "\n",
    "# compute the set B(x)= {i: x_i = 0, gradf(x)_i > 0}\n",
    "# the function returns an indicator vector. \n",
    "function set_B(A, b, x)\n",
    "    cond1 = x.==0\n",
    "    cond2 = grad(A, b, x).>0\n",
    "    return cond1.&&cond2 \n",
    "end\n",
    "\n",
    "# the stopping criterion for the (outer) while loop \n",
    "function stopping_criterion_not_met(A, b, x, epsilon)\n",
    "    indices_in_B = set_B(A, b, x)\n",
    "    indices_to_check = .!indices_in_B\n",
    "    abs_gradf_at_x = abs.(grad(A, b, x))\n",
    "    return (maximum(indices_to_check.*abs_gradf_at_x)<epsilon)\n",
    "end\n",
    "    \n",
    "# compute alpha \n",
    "function compute_alpha(A, b, x)\n",
    "    gradf_tilde = grad(A, b, x).*(.!set_B(A, b, x))\n",
    "    alpha = norm(gradf_tilde)^2/norm(A*gradf_tilde)^2\n",
    "    return alpha \n",
    "end\n",
    "    \n",
    "# projection onto nonnegative orthant\n",
    "function projection_step(input_vec)\n",
    "    indices = input_vec.>=0\n",
    "    return input_vec.*indices \n",
    "end\n",
    "    \n",
    "# check descent condition \n",
    "function descent_condition_satisfied(A, b, xkm, xhat, sigma)\n",
    "    fxc  = fn_val(A, b, xkm)\n",
    "    fxcM = fn_val(A, b, xhat)\n",
    "    gradf_xc = grad(xkm)\n",
    "    check_cond = fxc - fxcM - sigma*dot(gradf_xc, xkm - xhat) > 0\n",
    "    return check_cond\n",
    "end\n",
    "\n",
    "# main algorithm below, subroutines above\n",
    "function algorithm_kim_sra_dhillon(A, b, m, n, epsilon)\n",
    "    (xkm, xk) = ksd_init(n) #ksd = author initials\n",
    "    \n",
    "    eta = 0.5 #unspecified [TBD]\n",
    "    beta = 0.9 #unspecified by KSD [TBD]\n",
    "    M = 1000 # [TBD]\n",
    "    sigma = 0.1 # [TBD]\n",
    "    \n",
    "    while stopping_criterion_not_met(A, b, x, epsilon)\n",
    "        xhatm = deepcopy(xkm)\n",
    "        xhat = deepcopy(xk)\n",
    "        \n",
    "        for j in 1:M #subspace Barzilai-Borwein \n",
    "            alpha = compute_alpha(A, b, x)\n",
    "            xhatm = deepcopy(xhat)\n",
    "            w = xhat - beta*alpha*grad(xhat) \n",
    "            xhat.= projection_step(w)\n",
    "        end\n",
    "        \n",
    "        if descent_condition_satisfied(A, b, xkm, xhat, sigma)\n",
    "            xkm = deepcopy(xk)\n",
    "            xk = deepcopy(xhat)\n",
    "        else\n",
    "            beta = eta*beta \n",
    "        end\n",
    "    end\n",
    "    print(\"\\n KSD value is \", fn_val(xk))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code is the main function that generates data and calls different algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Our value is 2.8625317834852746  3.081644 seconds (4.74 M allocations: 241.756 MiB, 5.71% gc time, 99.93% compilation time)\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ ./timing.jl:220 [inlined]",
      " [2] top-level scope",
      "   @ ./In[6]:0",
      " [3] eval",
      "   @ ./boot.jl:373 [inlined]",
      " [4] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "# Main code\n",
    "epsilon = 0.001 \n",
    "n = 10 # variable dimension \n",
    "m = 50 # Number of data points\n",
    "\n",
    "# b can also be random and negative. m<<n.\n",
    "\n",
    "b=rand(m,1)-repeat([0.3],m,1)\n",
    "A=rand(m,n)\n",
    "remove_col1(A,b) #A is modified in memory, therefore not returned\n",
    "(m,n)=size(A) # Redefine the size number n and m to prevent triviality.\n",
    "\n",
    "@time begin\n",
    "our_result = algorithm_ours(epsilon, m, n, b, A)\n",
    "end \n",
    "\n",
    "@time begin\n",
    "algorithm_convex_dot_jl(A, b, n)\n",
    "end\n",
    "\n",
    "@time begin \n",
    "algorithm_kim_sra_dhillon(A, b, m, n, epsilon)\n",
    "end\n",
    "\n",
    "f = plot(our_result)\n",
    "@show f \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.0",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
