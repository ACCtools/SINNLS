{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " nnls package value is 0.04143643249310306, and time is 652.985942 seconds (38.78 k allocations: 302.835 MiB, 0.02% gc time, 0.01% compilation time)\n",
      "\n",
      "\n",
      " Obj = 0.04143722175176279, time since init = 240.51501488685608, new metric = 1.1717687324093588e-8\n",
      "Just to check, here's new_metric 1.1717687324093588e-8\n",
      "Just checking ONE MORE TIME, 1.1717687324093588e-8\n",
      "\n",
      "\n",
      "\n",
      " Our result with restart is 0.04143722175176279, and time is 240.690256 seconds (2.11 G allocations: 283.281 GiB, 11.88% gc time)\n"
     ]
    }
   ],
   "source": [
    "using LinearAlgebra, NonNegLeastSquares\n",
    "\n",
    "function alg_ours_with_restart(C::Matrix{Float64}, b::Matrix{Float64}, ϵ::Float64 )\n",
    "    extra_term_nnls = 0.5*norm(b)^2\n",
    "    m, n = size(C)\n",
    "    #number of times we restart\n",
    "    K = 50 #actually doesn't matter, since the restart_metric becomes really tiny pretty quickly \n",
    "    col_norm = norm.(eachcol(C))\n",
    "    inv_col_norm_square = 1.0 ./(col_norm.^2)\n",
    "    idx_seq = 1:n\n",
    "\n",
    "    x0 = zeros(n)\n",
    "    y0 = zeros(m)\n",
    "    z0 = zeros(m)\n",
    "\n",
    "    gamma = 30 #chosen after experiments on synthetic data\n",
    "    obj = 0 \n",
    "    \n",
    "    init_time = time()\n",
    "    for i=1:K\n",
    "\n",
    "        xktilde, yktilde, zktilde, new_metric, obj = alg_our_core(C, x0, y0, z0, m, n, inv_col_norm_square, idx_seq, ϵ, gamma, init_time, extra_term_nnls)\n",
    "\n",
    "        if (new_metric < ϵ)\n",
    "            break\n",
    "        end\n",
    "        x0[:] = xktilde[:]\n",
    "        y0[:] = yktilde[:]\n",
    "        z0[:] = zktilde[:]\n",
    "\n",
    "        gamma/= 2 # chosen after experiments on synthetic data \n",
    "    end\n",
    "    return obj\n",
    "end\n",
    "\n",
    "function alg_our_core(C, x0, y0, z0, m, n, inv_col_norm_square, idx_seq, ϵ, gamma, init_time, extra_term_nnls)\n",
    "        # reset all the scaling factors\n",
    "        previous_A = 1.0/n\n",
    "        previous_a = previous_A #a_1, A_1\n",
    "        a = 1.0/(n*n) # a_2\n",
    "        A = (n+1.0)/(n * n) # A_2\n",
    "\n",
    "\n",
    "        # compute x1 using the input x0\n",
    "        # we redefined phio(x) = 1/2 * ||x-x0||_A^2, hence updating x requires x0\n",
    "        # the step p(j)+=1/||A:j||^2 implicitly assumes ybar_0 = 0\n",
    "        # To allow for ybar_0 \\neq 0, we change p(j) a bit\n",
    "\n",
    "        ybar = copy(y0)\n",
    "        j = rand(idx_seq)\n",
    "        Aty0m = 1 - dot(ybar, C[:, j]) #                                     dot(̄ȳ, C[:, j])\n",
    "\n",
    "        p = copy(x0)\n",
    "        x = copy(x0)\n",
    "        p[j] += inv_col_norm_square[j]*Aty0m\n",
    "        x[j] = min(inv_col_norm_square[j], max(0, p[j])) #x and x0 differ only at j\n",
    "\n",
    "        # compute y1\n",
    "        # note that y0^(R) and y1^(R) are independent of each othre\n",
    "        # y1^(R) = Ax1^(R) = Ax0^(R) + A*(x1^R - x0^R) = z0^R + A*(x1^R - x0^R)\n",
    "        # y0^R may be chosen to be either ytildeK or 0 (our analysis uses 0)\n",
    "        # Further note that if y0^R = ytildeK, then we must ALSO choose ybar_0 = ytildeK, and\n",
    "        # this changes how x is init.\n",
    "        previous_y = copy(y0)\n",
    "        z = copy(z0)\n",
    "        z += C[:, j] * (x[j] - x0[j]) # z_1 = A x_1 = A (x_0 + (x_1 - x_0))\n",
    "        y = copy(z) # y_1 = A xtilde1 = A x_1 = z_1\n",
    "\n",
    "        # compute ȳ, ỹ (because we need to return it), and some auxiliary variables\n",
    "        ybar[:] = y[:] + previous_a/a * (y[:] - previous_y[:]) #ybar_1\n",
    "        s = zeros(n) # need this so that xtildek = xk + sk/Ak; s_1 = 0 (see Chaobing's lemma for why this is needed)\n",
    "        ỹ = copy(y) # ytildek = convex comb of yi's, so ytilde1 = y1\n",
    "\n",
    "        # restart value init; -1^{\\top}x+0.5\\|Ax\\|^{2}+.5*\\|y\\|^{2}+\\frac{1}{2\\epsilon}\\|(-A^{\\top}y+1)^{+}\\|^{2}\n",
    "        restart_coeff = 5000\n",
    "        Atym = -C'*y0 .+ 1\n",
    "        truncated_Atym = ((Atym) .> 0).*Atym\n",
    "        restart_val_prev = -sum(x0)+ 0.5* norm(z0)^2 +0.5*norm(y0)^2 + restart_coeff*norm(truncated_Atym)^2\n",
    "        restart_val_curr = restart_val_prev\n",
    "\n",
    "        # inits for restart\n",
    "        iter_count = 0\n",
    "        Flag = true\n",
    "        Ax0 = zeros(m)\n",
    "        new_metric = 0 \n",
    "        obj = 0 \n",
    "    \n",
    "        while (Flag)\n",
    "\n",
    "            # updates related to x\n",
    "            j = rand(idx_seq)\n",
    "            p[j] += - n * inv_col_norm_square[j] * a * (sum(C[:,j] .* ybar) - 1)\n",
    "            prev_xj = x[j]\n",
    "            x[j] = min(inv_col_norm_square[j], max(0, p[j]))\n",
    "            # update s so that we may return xtildek at only O(1) cost\n",
    "            s[j] += ((n-1) * a -  previous_A) * (x[j] - prev_xj)\n",
    "\n",
    "            # updates related to y\n",
    "            previous_y[:] = y[:]\n",
    "            z[:] += C[:, j] * (x[j] - prev_xj)\n",
    "            y[:] = previous_A/A * y[:] + a/A * z[:] + (n-1) * a/A * (x[j] - prev_xj) * C[:,j]\n",
    "            # need to update ytilde each time because that's what we want to return,\n",
    "            # and we aren't saving all the yi's.\n",
    "            ỹ[:] = previous_A/A * ỹ[:] + a/A * y[:]\n",
    "\n",
    "            # update scaling factors\n",
    "            previous_a, previous_A = a, A\n",
    "            a = min(n * a/(n-1), sqrt(A)/(2*n))\n",
    "            A += a\n",
    "\n",
    "            # update ȳ (note that ȳ_k depends on a_k and a_{k+1})\n",
    "            ybar[:] = y[:] + previous_a/a * (y[:] - previous_y[:])\n",
    "\n",
    "            # restart stuff\n",
    "            iter_count+=1\n",
    "            # Since we are computing the restart condition without any optimizations,\n",
    "            # and the restart condition likely involves (expensive) matrix-vector products,\n",
    "            # we check it only after a certain number of iters have passed.\n",
    "\n",
    "            if (iter_count % ceil(n*gamma) ==0)\n",
    "                # compute the restart metric \n",
    "                # note that we DO use the restart metric to terminate the OUTER ALG, \n",
    "                # , even though we are doing fixed restarts in the INNER ALG \n",
    "                Atym = -C'*ỹ .+ 1\n",
    "                truncated_Atym = ((Atym) .> 0).*Atym\n",
    "                sumx0 = sum(x + (1.0/previous_A) * s)\n",
    "                Ax0 = C*(x + (1.0/previous_A) * s)\n",
    "                new_metric = norm(truncated_Atym)^2\n",
    "\n",
    "                # For now, we are doing fixed restarts.\n",
    "                obj = extra_term_nnls-sumx0+ 0.5* norm(Ax0)^2\n",
    "                print(\"\\n\\n Obj = \", obj, \", time since init = \", time() - init_time, \", new metric = \", new_metric)\n",
    "                print(\"\\n\")\n",
    "                Flag = false\n",
    "                # end\n",
    "            end\n",
    "        end\n",
    "        return x + (1.0/previous_A) * s, ỹ, Ax0, new_metric, obj\n",
    "end\n",
    "\n",
    "function alg_ours_without_restart(C::Matrix{Float64}, b::Matrix{Float64}, ϵ::Float64)\n",
    "    \n",
    "    extra_term_nnls = 0.5*norm(b)^2\n",
    "    m, n = size(C)\n",
    "    K = ceil(n / √ϵ)\n",
    "    previous_A = 1.0/n\n",
    "    previous_a = previous_A\n",
    "    a = 1.0/(n*n)\n",
    "    A = (n+1.0) /(n * n)\n",
    "    col_norm = norm.(eachcol(C))\n",
    "    inv_col_norm_square = 1.0 ./(col_norm.^2)\n",
    "    idx_seq = 1:n\n",
    "    x = zeros(n)\n",
    "    p = zeros(n)\n",
    "    j = rand(idx_seq)\n",
    "    p[j] += inv_col_norm_square[j]\n",
    "    x[j] = p[j]\n",
    "    # x̃ = deepcopy(x)\n",
    "    previous_y = zeros(m)\n",
    "    y = x[j] * C[:, j]\n",
    "    # record Ax\n",
    "    z = x[j] * C[:, j]\n",
    "    ȳ = (n+1) * y\n",
    "    s = zeros(n)\n",
    "    #func_value = 0\n",
    "    func_value=zeros(Int(ceil(K/n)))\n",
    "    \n",
    "    for k = 2:K\n",
    "        j = rand(idx_seq)\n",
    "        p[j] += - n * inv_col_norm_square[j] * a * (sum(C[:,j] .* ȳ) - 1)\n",
    "        prev_xj = x[j]\n",
    "        x[j] = min(inv_col_norm_square[j], max(0, p[j]))\n",
    "        # record Ax\n",
    "        z[:] += C[:, j] * (x[j] - prev_xj)\n",
    "        previous_y[:] = y[:]\n",
    "        y[:] = previous_A/A * y[:] + a/A * z[:] + (n-1) * a/A * (x[j] - prev_xj) * C[:,j]\n",
    "        s[j] += ((n-1) * a -  previous_A) * (x[j] - prev_xj)\n",
    "        previous_a, previous_A = a, A\n",
    "        a = min(n * a/(n-1), sqrt(A)/(2*n))\n",
    "        ȳ[:] = y[:] + previous_a/a * (y[:] - previous_y[:])\n",
    "        A += a\n",
    "    end\n",
    "    x̃ = x + 1.0/previous_A * s\n",
    "    C_x̃ = C * x̃\n",
    "    func_value = 0.5 * sum(C_x̃ .* C_x̃) - sum(x̃)+extra_term_nnls\n",
    "\n",
    "    return(func_value)\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "# https://github.com/ahwillia/NonNegLeastSquares.jl\n",
    "function alg_lawsonhanson(A, b)\n",
    "    xnnls = nonneg_lsq(A,b;alg=:nnls)  # NNLS\n",
    "\n",
    "    nnls_optval = 0.5*norm(A*xnnls - b)^2\n",
    "    \n",
    "    print(\"\\n\\n nnls package value is \", nnls_optval, \", and time is \")\n",
    "end\n",
    "\n",
    "# Main code\n",
    "function remove_col1(A,b)#Chenghui has an idea to optimize this for speed (\"filter\")\n",
    "    s=A'*b # n*1\n",
    "    B=A[:,vec(s.>0)] # m*b matrix where b is smaller than n\n",
    "    s=s[vec(s.>0)] # s is b*1 in dimensions\n",
    "    return B./s'\n",
    "end\n",
    "\n",
    "epsilon = 0.001\n",
    "\n",
    "#############\n",
    "n = 3000000 # variable dimension\n",
    "m = 10 # Number of data points\n",
    "b=rand(m,1)-repeat([0.3],m,1)\n",
    "A_init =  max.(0, randn(m, n)) #rand(m,n)#\n",
    "############\n",
    "\n",
    "############ Mnist： uncomment\n",
    "#train_x, train_y = MNIST.traindata()\n",
    "#A = Array{Int64}\n",
    "#b = Array{Int64}\n",
    "#A_init = reshape(train_x,60000,28*28)\n",
    "#b = train_y\n",
    "#test_x,  test_y  = MNIST.testdata()\n",
    "############\n",
    "\n",
    "\n",
    "A = remove_col1(A_init,b)\n",
    "############ Mnist: uncomment this\n",
    "# A = Float64.(A)\n",
    "# b = Float64.(b)\n",
    "# b1 = vcat(b')\n",
    "############\n",
    "#\n",
    "@time begin\n",
    "alg_lawsonhanson(A, b)\n",
    "end\n",
    "\n",
    "# @time begin\n",
    "# our_result = alg_ours_without_restart(A, b, epsilon)\n",
    "#     print(\"\\n\\n Our result without restart is \", our_result, \", and time is \")\n",
    "# #our_result = alg_ours(A, b1, epsilon)\n",
    "# end\n",
    "\n",
    "@time begin\n",
    "our_result = alg_ours_with_restart(A, b, epsilon)\n",
    "print(\"\\n\\n Our result with restart is \", our_result, \", and time is \")\n",
    "#our_result = alg_ours(A, b1, epsilon)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.0",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
